import cv2
import torch
import numpy as np
 
import tempfile
import subprocess
import os

class SplitVideoByFrames:
    """
    ç”¨OpenCVæ‹†åˆ†è§†é¢‘ä¸ºå¤šä¸ªç‰‡æ®µï¼Œæ¯æ®µå¸§æ•°ä¸è¶…è¿‡max_frames_per_clipï¼ŒéŸ³é¢‘è¾“å‡ºä¸ºComfyUIå®˜æ–¹æ ¼å¼
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video_path": ("STRING", {"default": "your_video.mp4"}),
                "max_frames_per_clip": ("INT", {"default": 30, "min": 1, "max": 1000}),
            }
        }

    RETURN_TYPES = ("INT", "LIST", "AUDIO")
    RETURN_NAMES = ("num_clips", "clips", "audio")
    FUNCTION = "split_video"
    CATEGORY = "äº‘æœåŠ¡"

    def split_video(self, video_path, max_frames_per_clip):
        # 1. æå–éŸ³é¢‘ä¸ºwavä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_audio:
            audio_path = tmp_audio.name
        ffmpeg_bin = "ffmpeg"  # å‡è®¾å·²åœ¨ç¯å¢ƒå˜é‡
        cmd = [ffmpeg_bin, '-y', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '44100', '-ac', '2', '-f', 'wav', audio_path]
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        if not os.path.exists(audio_path) or os.path.getsize(audio_path) == 0:
            print(f"è­¦å‘Šï¼šè§†é¢‘ {video_path} æ— éŸ³è½¨æˆ–éŸ³é¢‘æå–å¤±è´¥ï¼Œè¿”å›ç©ºéŸ³é¢‘ã€‚")
            waveform = torch.zeros((1, 2, 1), dtype=torch.float32)  # 1å¸§2é€šé“ç©ºéŸ³é¢‘
            sample_rate = 44100
            audio_dict = {"waveform": waveform, "sample_rate": sample_rate}
        else:
            try:
                import torchaudio
                waveform, sample_rate = torchaudio.load(audio_path)
            except Exception as e:
                print(f"è­¦å‘Šï¼šéŸ³é¢‘æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œè¿”å›ç©ºéŸ³é¢‘ã€‚é”™è¯¯ä¿¡æ¯: {e}")
                waveform = torch.zeros((1, 2, 1), dtype=torch.float32)
                sample_rate = 44100
            audio_dict = {"waveform": waveform, "sample_rate": sample_rate}
        os.remove(audio_path)
        # ä¿è¯shapeä¸º(1,channels,samples)
        if waveform.dim() == 2:
            waveform = waveform.unsqueeze(0)
        audio_dict = {"waveform": waveform, "sample_rate": sample_rate}

        # 3. è§†é¢‘åˆ†å¸§ï¼ˆä¸åšresizeï¼Œå‡è®¾è§†é¢‘åˆ†è¾¨ç‡ä¸€è‡´ï¼‰
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise FileNotFoundError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        frames = []
        clips = []
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            np_image = frame.astype(np.float32) / 255.0
            frames.append(torch.from_numpy(np_image))
            if len(frames) == max_frames_per_clip:
                clips.append({"frames": torch.stack(frames, dim=0), "audio": audio_dict})
                frames = []
        if frames:
            clips.append({"frames": torch.stack(frames, dim=0), "audio": audio_dict})
        cap.release()
        return (len(clips), clips, audio_dict)

class GetVideoClipByIndex:
    """
    è¾“å…¥clipså’Œç´¢å¼•ï¼Œè¾“å‡ºè¯¥ç‰‡æ®µçš„å›¾ç‰‡æ•°ç»„ï¼ˆIMAGEï¼‰ã€éŸ³é¢‘ï¼ˆå®˜æ–¹æ ¼å¼ï¼‰å’Œå¸§æ•°ï¼ˆnum_framesï¼‰
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "index": ("INT", {"default": 0, "min": 0}),
                "clips": ("LIST", {}),
            }
        }

    RETURN_TYPES = ("IMAGE", "AUDIO", "INT")
    RETURN_NAMES = ("images", "audio", "num_frames")
    FUNCTION = "get_clip"
    CATEGORY = "äº‘æœåŠ¡"

    def get_clip(self, index, clips):
        if not clips or index < 0 or index >= len(clips):
            raise IndexError("ç´¢å¼•è¶…å‡ºclipsèŒƒå›´")
        clip = clips[index]
        frames = clip["frames"]
        audio = clip["audio"]
        print(f"è·å–ç‰‡æ®µ {index}ï¼Œå¸§æ•°: {frames.shape[0]}, éŸ³é¢‘é‡‡æ ·ç‡: {audio['sample_rate']}")
        num_frames = frames.shape[0]
        return (frames, audio, num_frames)

class CreateEmptyImageBatch:
    """
    åˆ›å»ºä¸€ä¸ªç©ºçš„å›¾åƒbatchï¼Œè‹¥ä¼ å…¥imagesåˆ™è¿½åŠ åˆ°ç©ºbatchåè¿”å›ã€‚
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "optional": {
                "images": ("IMAGE", {}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_batch",)
    FUNCTION = "create"
    CATEGORY = "äº‘æœåŠ¡/é›†åˆå·¥å…·"

    def create(self, images=None):
        import torch
        def to_3ch(img):
            if img is None:
                return img
            if img.dim() == 3:
                if img.shape[-1] == 1:
                    img = img.repeat(1, 1, 3)
                elif img.shape[-1] == 4:
                    img = img[..., :3]
            elif img.dim() == 4:
                if img.shape[-1] == 1:
                    img = img.repeat(1, 1, 1, 3)
                elif img.shape[-1] == 4:
                    img = img[..., :3]
            return img
        batch = torch.empty((0, 0, 0, 0), dtype=torch.float32)
        if images is not None:
            images = to_3ch(images)
            if images.dim() == 3:
                images = images.unsqueeze(0)
            # ç›´æ¥ç”¨AppendImagesToBatchçš„é€»è¾‘æ‹¼æ¥
            if batch.numel() == 0 or batch.shape[0] == 0:
                return (images,)
            else:
                new_batch = torch.cat([batch, images], dim=0)
                return (new_batch,)
        return (batch,)

class AppendImagesToBatch:
    """
    å‘å·²æœ‰å›¾åƒbatchè¿½åŠ å›¾ç‰‡ï¼Œæ”¯æŒå•å¼ æˆ–å¤šå¼ å›¾ç‰‡ï¼Œè¾“å‡ºåˆå¹¶åçš„IMAGE batchã€‚
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image_batch": ("IMAGE", {}),
                "images_to_add": ("IMAGE", {}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_batch",)
    FUNCTION = "append"
    CATEGORY = "äº‘æœåŠ¡/é›†åˆå·¥å…·"

    def append(self, image_batch, images_to_add):
        import torch
        # å¦‚æœ image_batch æ˜¯ç©ºçš„ï¼ˆshapeä¸º(0, 0, 0, 0)ï¼‰ï¼Œç›´æ¥è¿”å› images_to_add
        if image_batch.numel() == 0 or image_batch.shape[0] == 0:
            if images_to_add.dim() == 3:
                images_to_add = images_to_add.unsqueeze(0)
            return (images_to_add,)
        if image_batch.dim() == 3:
            image_batch = image_batch.unsqueeze(0)
        if images_to_add.dim() == 3:
            images_to_add = images_to_add.unsqueeze(0)
        new_batch = torch.cat([image_batch, images_to_add], dim=0)
        return (new_batch,)

class GetFirstImageFromBatch:
    """
    è·å–å›¾ç‰‡batchçš„é¦–å¸§æˆ–æœ«å¸§ï¼Œè¾“å‡ºå•å¸§IMAGE batchï¼ˆshapeä¸º(1, H, W, C)ï¼‰åŠå®½é«˜ã€‚
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image_batch": ("IMAGE", {}),
                "mode": (["first", "last"], {"default": "first"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "INT", "INT")
    RETURN_NAMES = ("image", "width", "height")
    FUNCTION = "get"
    CATEGORY = "äº‘æœåŠ¡/é›†åˆå·¥å…·"

    def get(self, image_batch, mode="first"):
        import torch
        if image_batch.dim() == 3:
            img = image_batch.unsqueeze(0)
        else:
            if image_batch.shape[0] == 0:
                raise ValueError("image_batch is empty!")
            if mode == "first":
                img = image_batch[0:1]
            else:
                img = image_batch[-1:]
        # img shape: (1, H, W, C)
        _, h, w, _ = img.shape
        return (img, w, h)

class RemoveFirstOrLastImageFromBatch:
    """
    åˆ é™¤å›¾ç‰‡batchçš„é¦–å¸§æˆ–æœ«å¸§ï¼Œè¾“å‡ºå‰©ä½™IMAGE batchã€‚
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image_batch": ("IMAGE", {}),
                "mode": (["first", "last"], {"default": "first"}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_batch",)
    FUNCTION = "remove"
    CATEGORY = "äº‘æœåŠ¡/é›†åˆå·¥å…·"

    def remove(self, image_batch, mode="first"):
        import torch
        if image_batch.dim() == 3:
            # åªæœ‰ä¸€å¼ ï¼Œåˆ é™¤åä¸ºç©ºbatch
            return (torch.empty((0, *image_batch.shape), dtype=image_batch.dtype, device=image_batch.device),)
        if image_batch.shape[0] == 0:
            return (image_batch,)
        if mode == "first":
            return (image_batch[1:],)
        else:
            return (image_batch[:-1],)

NODE_CLASS_MAPPINGS = {
    "SplitVideoByFrames": SplitVideoByFrames,
    "GetVideoClipByIndex": GetVideoClipByIndex,
    "CreateEmptyImageBatch": CreateEmptyImageBatch,
    "AppendImagesToBatch": AppendImagesToBatch,
    "GetFirstImageFromBatch": GetFirstImageFromBatch,
    "RemoveFirstOrLastImageFromBatch": RemoveFirstOrLastImageFromBatch
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "SplitVideoByFrames": "ğŸ¬ è§†é¢‘åˆ†æ®µæ‹†å¸§",
    "GetVideoClipByIndex": "ğŸ–¼ï¸ è·å–ç‰‡æ®µå›¾ç‰‡å’ŒéŸ³é¢‘",
    "CreateEmptyImageBatch": "ğŸ“‚ åˆ›å»ºç©ºå›¾åƒBatch",
    "AppendImagesToBatch": "â• è¿½åŠ å›¾ç‰‡åˆ°Batch",
    "GetFirstImageFromBatch": "ğŸ” è·å–é¦–/æœ«å¸§å›¾ç‰‡",
    "RemoveFirstOrLastImageFromBatch": "âŒ åˆ é™¤é¦–/æœ«å¸§å›¾ç‰‡"
}