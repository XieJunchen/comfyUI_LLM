import requests
import json
import logging
import re
import os
import threading
from threading import Lock
from typing import Optional, List, Union, Dict, Any

# ----------------------------
# æ—¥å¿—ç³»ç»Ÿé…ç½®
# ----------------------------
desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
log_path = os.path.join(desktop_path, "ollama_comfyui.log")

# å¼ºåˆ¶åˆ é™¤æ—§æ—¥å¿—
try:
    if os.path.exists(log_path):
        os.remove(log_path)
except Exception as e:
    print(f"æ—§æ—¥å¿—åˆ é™¤å¤±è´¥: {str(e)}")

# æ—¥å¿—åˆå§‹åŒ–
logging.basicConfig(
    filename=log_path,
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    filemode='w'
)
logger = logging.getLogger(__name__)
logger.addHandler(logging.StreamHandler())  # æ§åˆ¶å°è¾“å‡º

class ComfyUI_LLM_Ollama:
    """
    Ollama LLMé›†æˆèŠ‚ç‚¹
    
    ç‰¹æ€§ï¼š
    - æ”¯æŒæµå¼å“åº”ç”Ÿæˆ
    - ä¸Šä¸‹æ–‡è®°å¿†ç®¡ç†
    - å¤šçº¿ç¨‹å®‰å…¨è®¿é—®
    - åŠ¨æ€æ¨¡å‹åˆ—è¡¨åŠ è½½
    - è¯¦ç»†çš„æ—¥å¿—è®°å½•
    """
    # åœ¨ç±»å®šä¹‰é¡¶éƒ¨æ·»åŠ 
    WEB_DIRECTORY = "./js"
    
    # ç±»çº§å…±äº«çŠ¶æ€
    _conn_lock = Lock()
    _api_lock = Lock()
    _connection_checked = False
    _connection_status = False
    _available_models = ["llama3", "deepseek-r1:7b"]  # é»˜è®¤å€¼
    
    # ç±»çº§æ—¥å¿—å™¨ âœ… ä¿®æ­£ç‚¹1
    logger = logging.getLogger("ComfyUI-Ollama")

    @classmethod
    def INPUT_TYPES(cls):
        """åŠ¨æ€ç”Ÿæˆè¾“å…¥é…ç½®"""
        # åœ¨ç±»åŠ è½½æ—¶å°è¯•è·å–å¯ç”¨æ¨¡å‹
        if not cls._connection_checked:
            cls._check_connection()
        
        return {
            "required": {
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "è¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”...",
                    "dynamicPrompts": False,
                    "lineCount": 4  # âœ… åˆå§‹æ˜¾ç¤º4è¡Œé«˜åº¦
                }),
                "model": (cls._available_models, {"default": "deepseek-r1:7b"}),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider"
                }),
                "max_tokens": ("INT", {
                    "default": 1024,
                    "min": 1,
                    "max": 4096,
                    "step": 64,
                    "display": "number"
                }),
                "hide_thoughts": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "context": ("STRING", {"forceInput": True}),
                "system_message": ("STRING", {
                    "multiline": True,
                    "default": "ä½ æ˜¯æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹",
                    "lazy": True,
                    "lineCount": 3  # âœ… åˆå§‹æ˜¾ç¤º3è¡Œé«˜åº¦
                }),
                "stop_sequences": ("STRING", {
                    "default": "",
                    "lazy": True
                }),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("response", "context")
    FUNCTION = "generate"
    CATEGORY = "LLM"
    OUTPUT_NODE = False

    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.headers = {"Content-Type": "application/json"}
        self.timeout = 120
        self._init_logging()  # âœ… ä¿®æ­£ç‚¹2ï¼šå®ä¾‹åˆå§‹åŒ–æ—¥å¿—é…ç½®

    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        self.logger = logging.getLogger("ComfyUI-Ollama")
        self.logger.propagate = False
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('[%(name)s] %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
        self.logger.setLevel(logging.INFO)

    @classmethod
    def _check_connection(cls):
        """ç±»æ–¹æ³•ä½¿ç”¨ç±»çº§æ—¥å¿—å™¨ âœ… ä¿®æ­£ç‚¹3"""
        with cls._conn_lock:
            if not cls._connection_checked:
                try:
                    # ä½¿ç”¨ç±»çº§æ—¥å¿—å™¨
                    cls.logger.info(f"ğŸ›  æ­£åœ¨æ£€æŸ¥Ollamaè¿æ¥...")
                    
                    response = requests.get(
                        "http://localhost:11434/api/tags",
                        timeout=10
                    )
                    if response.status_code == 200:
                        models = response.json().get("models", [])
                        cls._available_models = [m["name"] for m in models]
                        cls._connection_status = True
                        cls.logger.info(f"âœ… å¯ç”¨æ¨¡å‹: {cls._available_models}")
                    else:
                        cls.logger.warning(f"è¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
                except Exception as e:
                    cls.logger.error(f"è¿æ¥å¼‚å¸¸ï¼š{str(e)}")
                finally:
                    cls._connection_checked = True

    def _build_payload(self, **kwargs):
        """æ„é€ APIè¯·æ±‚è´Ÿè½½"""
        stop_sequences = [s.strip() for s in kwargs['stop_sequences'].split(',')] if kwargs['stop_sequences'] else []
        
        return {
            "model": kwargs['model'],
            "prompt": kwargs['prompt'],
            "system": kwargs.get('system_message', 'ä½ æ˜¯æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹'),
            "context": self._parse_context(kwargs.get('context')),
            "options": {
                "temperature": kwargs['temperature'],
                "num_predict": kwargs['max_tokens'],
                "stop": stop_sequences,
            }
        }

    def _parse_context(self, context: Optional[str]) -> List:
        """è§£æä¸Šä¸‹æ–‡æ•°æ®"""
        try:
            return json.loads(context) if context else []
        except json.JSONDecodeError:
            self.logger.warning("ä¸Šä¸‹æ–‡è§£æå¤±è´¥ï¼Œä½¿ç”¨ç©ºä¸Šä¸‹æ–‡")
            return []

    def _clean_response(self, text: str, hide_thoughts: bool) -> str:
        """å“åº”åå¤„ç†"""
        if hide_thoughts:
            return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()
        return text.strip()

    def generate(self, **kwargs):
        """ä¸»æ‰§è¡Œæ–¹æ³•"""
        if not self._connection_status:
            return ("OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥", "")
            
        with self._api_lock:
            try:
                payload = self._build_payload(**kwargs)
                self.logger.debug(f"è¯·æ±‚å‚æ•°ï¼š{json.dumps(payload, indent=2)}")
                
                response_text = ""
                context = []
                
                with requests.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    headers=self.headers,
                    stream=True,
                    timeout=self.timeout
                ) as response:
                    response.raise_for_status()
                    
                    for line in response.iter_lines():
                        if line:
                            data = json.loads(line.decode('utf-8'))
                            response_text += data.get("response", "")
                            if data.get("done"):
                                context = data.get("context", [])
                
                cleaned_response = self._clean_response(response_text, kwargs['hide_thoughts'])
                self.logger.info(f"ğŸ“¥ å“åº”é•¿åº¦: {len(cleaned_response)}å­—ç¬¦")
                return (cleaned_response, json.dumps(context))
                
            except requests.RequestException as e:
                error_msg = f"APIè¯·æ±‚å¤±è´¥: {str(e)}"
                self.logger.error(error_msg)
                return (error_msg, "")
            except Exception as e:
                error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
                self.logger.exception(error_msg)
                return (error_msg, "")

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "ComfyUI_LLM_Ollama": ComfyUI_LLM_Ollama
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ComfyUI_LLM_Ollama": "ğŸ¤– Ollama LLM"
}