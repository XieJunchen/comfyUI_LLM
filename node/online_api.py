import os
import json
import logging
from typing import Optional, List
from openai import OpenAI, Stream
from openai.types.chat import ChatCompletionChunk

# ----------------------------
# æ—¥å¿—ç³»ç»Ÿé…ç½®
# ----------------------------
desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
log_path = os.path.join(desktop_path, "deepseek_comfyui.log")

logging.basicConfig(
    filename=log_path,
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    filemode='w'
)
logger = logging.getLogger("ComfyUI-DeepSeek")
logger.addHandler(logging.StreamHandler())

class ComfyUI_LLM_Online:
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "password": True}),
                "input_str": ("STRING", {"multiline": True, "dynamicPrompts": False}),
                "model": (["deepseek-chat", "deepseek-coder","deepseek-reasoner"], {"default": "deepseek-chat"}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.1}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 4096}),
                "stop_sequences": ("STRING", {"default": ""}),
                "stream_mode": (["enable", "disable"], {"default": "disable"}),
            },
            "optional": {
                "system_prompt": ("STRING", {"default": "ä½ æ˜¯æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹", "multiline": True}),
                "context": ("STRING", {"default": ""}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("response",)
    FUNCTION = "query_llm"
    CATEGORY = "LLM"
    OUTPUT_NODE = True

    def _validate_inputs(self, **kwargs):
        """å‚æ•°éªŒè¯"""
        if not kwargs.get("api_key"):
            raise ValueError("APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        
        if len(kwargs.get("input_str", "").strip()) < 2:
            raise ValueError("è¾“å…¥å†…å®¹è¿‡çŸ­ï¼ˆè‡³å°‘éœ€è¦2ä¸ªå­—ç¬¦ï¼‰")

        if not 0 <= kwargs.get("temperature", 0.7) <= 1:
            raise ValueError("æ¸©åº¦å‚æ•°éœ€åœ¨0-1ä¹‹é—´")

    def _build_messages(self, **kwargs):
        """æ„é€ æ¶ˆæ¯å†å²"""
        messages = []
        
        # ç³»ç»Ÿæç¤º
        if system_prompt := kwargs.get("system_prompt"):
            messages.append({"role": "system", "content": system_prompt})
        
        # ä¸Šä¸‹æ–‡å†å²
        if context := kwargs.get("context"):
            try:
                history = json.loads(context)
                if isinstance(history, list):
                    messages.extend(history)
            except json.JSONDecodeError:
                logger.warning("ä¸Šä¸‹æ–‡è§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²")
                messages.append({"role": "user", "content": context})
        
        # å½“å‰è¾“å…¥
        messages.append({"role": "user", "content": kwargs["input_str"]})
        return messages

    def _handle_stream_response(self, stream: Stream[ChatCompletionChunk]) -> str:
        """å¤„ç†æµå¼å“åº”"""
        full_response = []
        for chunk in stream:
            if content := chunk.choices[0].delta.content:
                full_response.append(content)
                yield content  # å®æ—¶è¾“å‡º
        
        logger.debug(f"å®Œæ•´å“åº”ï¼š{''.join(full_response)}")
        return ''.join(full_response)

    def query_llm(self, **kwargs):
        try:
            # è¾“å…¥éªŒè¯
            self._validate_inputs(**kwargs)
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            client = OpenAI(
                api_key=kwargs["api_key"],
                base_url="https://api.deepseek.com/v1",
            )
            
            # æ„é€ è¯·æ±‚å‚æ•°
            stop_sequences = [s.strip() for s in kwargs["stop_sequences"].split(",") if s.strip()]
            
            # åˆ›å»ºAPIè¯·æ±‚
            response = client.chat.completions.create(
                model=kwargs["model"],
                messages=self._build_messages(**kwargs),
                temperature=kwargs["temperature"],
                max_tokens=kwargs["max_tokens"],
                stop=stop_sequences if stop_sequences else None,
                stream=kwargs["stream_mode"] == "enable",
            )
            
            # å¤„ç†å“åº”
            if isinstance(response, Stream):
                # æµå¼å¤„ç†
                return (self._handle_stream_response(response),)
            
            # æ™®é€šå“åº”
            content = response.choices[0].message.content
            logger.debug(f"APIå“åº”ï¼š{content}")
            return (content,)
            
        except Exception as e:
            logger.error(f"APIè¯·æ±‚å¤±è´¥ï¼š{str(e)}", exc_info=True)
            return (f"é”™è¯¯ï¼š{str(e)}",)

    @classmethod
    def IS_CHANGED(cls, **kwargs):
        """é€šè¿‡å“ˆå¸Œå€¼æ£€æµ‹è¾“å…¥å˜åŒ–"""
        return hash(json.dumps(kwargs, sort_keys=True))

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "DeepSeek_Online": ComfyUI_LLM_Online
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "DeepSeek_Online": "ğŸ§  DeepSeek æ™ºèƒ½åŠ©æ‰‹"
}