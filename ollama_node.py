import requests
import json
import logging
import re
import os
import threading
from threading import Lock
from typing import Optional, List, Union, Dict, Any

# ----------------------------
# æ—¥å¿—ç³»ç»Ÿé…ç½®
# ----------------------------
desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
log_path = os.path.join(desktop_path, "ollama_comfyui.log")

# å¼ºåˆ¶åˆ é™¤æ—§æ—¥å¿—
try:
    if os.path.exists(log_path):
        os.remove(log_path)
except Exception as e:
    print(f"æ—§æ—¥å¿—åˆ é™¤å¤±è´¥: {str(e)}")

# æ—¥å¿—åˆå§‹åŒ–
logging.basicConfig(
    filename=log_path,
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    filemode='w'
)
logger = logging.getLogger(__name__)
logger.addHandler(logging.StreamHandler())  # æ§åˆ¶å°è¾“å‡º

# ----------------------------
# æ ¸å¿ƒå®ç°ç±»
# ----------------------------
class ComfyUI_LLM_Ollama:
    # ç±»çº§å…±äº«çŠ¶æ€
    _conn_lock = Lock()     # è¿æ¥æ£€æŸ¥é”
    _api_lock = Lock()      # APIè¯·æ±‚é”
    _connection_checked = False
    _connection_status = False
    _available_models = []
    
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.headers = {"Content-Type": "application/json"}
        self.timeout = 120
        self._init_log_once()  # ç¡®ä¿æ—¥å¿—æç¤ºåªå‡ºç°ä¸€æ¬¡

    def _init_log_once(self):
        """ç¡®ä¿æ—¥å¿—è·¯å¾„æç¤ºåªæ‰“å°ä¸€æ¬¡"""
        if not hasattr(self.__class__, '_log_initialized'):
            logger.info(f"ğŸ“Œ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_path}")
            print(f"ğŸ‘‰ æ—¥å¿—è·¯å¾„: {log_path} [ä»…é¦–æ¬¡æç¤º]")
            self.__class__._log_initialized = True

    # ----------------------------
    # è¿æ¥ç®¡ç†
    # ----------------------------
    def _check_connection(self) -> bool:
        """çº¿ç¨‹å®‰å…¨çš„è¿æ¥æ£€æŸ¥ï¼ˆå¸¦åŒé‡æ£€æŸ¥é”å®šï¼‰"""
        if self.__class__._connection_checked:
            return self.__class__._connection_status
            
        with self.__class__._conn_lock:
            if not self.__class__._connection_checked:
                try:
                    logger.debug(f"ğŸ›  [{threading.current_thread().name}] æ­£åœ¨å»ºç«‹åˆå§‹è¿æ¥...")
                    resp = requests.get(f"{self.ollama_url}/api/tags", timeout=15)
                    models = resp.json().get("models", [])
                    self.__class__._available_models = [m["name"] for m in models]
                    self.__class__._connection_status = True
                    logger.info(f"âœ… å¯ç”¨æ¨¡å‹: {', '.join(self._available_models)}")
                except Exception as e:
                    logger.error(f"âŒ è¿æ¥å¤±è´¥: {str(e)}")
                    self.__class__._connection_status = False
                finally:
                    self.__class__._connection_checked = True
        return self.__class__._connection_status

    # ----------------------------
    # æ ¸å¿ƒå¤„ç†é€»è¾‘
    # ----------------------------
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {"multiline": True, "default": "è¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”..."}),
                "model": (["llama3", "deepseek-r1:7b", "mistral", "phi3"], {"default": "deepseek-r1:7b"}),
                "hide_thoughts": ("BOOLEAN", {"default": False}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0, "step": 0.1}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 4096, "step": 1}),
            },
            "optional": {
                "context": ("STRING", {"forceInput": True}),
                "system_message": ("STRING", {"multiline": True, "default": "ä½ æ˜¯æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"}),
                "stop_sequences": ("STRING", {"default": ""}),
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("response", "context")
    FUNCTION = "generate_response"
    CATEGORY = "LLM"

    def _parse_context(self, context: Optional[Union[str, List]]) -> List:
        """è§£æä¸Šä¸‹æ–‡æ•°æ®"""
        if not context:
            return []
        try:
            if isinstance(context, str):
                return json.loads(context) if context.strip() else []
            return list(context)
        except Exception as e:
            logger.warning(f"ä¸Šä¸‹æ–‡è§£æå¤±è´¥: {str(e)}")
            return []

    def _clean_thoughts(self, text: str) -> str:
        """æ¸…ç†æ€è€ƒè¿‡ç¨‹ï¼ˆæ”¯æŒè·¨è¡ŒåŒ¹é…ï¼‰"""
        return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()

    def generate_response(self, 
                        prompt: str,
                        model: str,
                        hide_thoughts: bool,
                        temperature: float,
                        max_tokens: int,
                        context: Optional[str] = None,
                        system_message: Optional[str] = None,
                        stop_sequences: Optional[str] = None) -> tuple:
        """çº¿ç¨‹å®‰å…¨çš„å“åº”ç”Ÿæˆå…¥å£"""
        # è¿æ¥æ£€æŸ¥
        if not self._check_connection():
            return ("âš ï¸ è¿æ¥å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡", "[]")
            
        # APIè¯·æ±‚ä¸´ç•ŒåŒºä¿æŠ¤
        with self.__class__._api_lock:
            thread_name = threading.current_thread().name
            logger.debug(f"ğŸš¦ [{thread_name}] è¿›å…¥APIä¸´ç•ŒåŒº")
            
            try:
                # æ„å»ºè¯·æ±‚å‚æ•°
                stop_list = [s.strip() for s in stop_sequences.split(",")] if stop_sequences else []
                payload = {
                    "model": model,
                    "prompt": prompt,
                    "system": system_message or "ä½ æ˜¯æœ‰å¸®åŠ©çš„AIåŠ©æ‰‹",
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens,
                        "stop": stop_list,
                    },
                    "context": self._parse_context(context),
                }
                logger.debug(f"ğŸ“¤ è¯·æ±‚è´Ÿè½½: {json.dumps(payload, indent=2, ensure_ascii=False)}")

                # æµå¼å¤„ç†å“åº”
                final_response = ""
                response_context = []
                try:
                    with requests.post(
                        f"{self.ollama_url}/api/generate",
                        headers=self.headers,
                        json=payload,
                        timeout=self.timeout,
                        stream=True
                    ) as resp:
                        resp.raise_for_status()
                        
                        for line in resp.iter_lines():
                            if line:
                                data = json.loads(line.decode('utf-8'))
                                if data.get("done", False):
                                    break
                                if "response" in data:
                                    final_response += data["response"]
                                if "context" in data:
                                    response_context = data["context"]
                                
                except requests.RequestException as e:
                    logger.error(f"ğŸ”´ è¯·æ±‚å¤±è´¥: {str(e)}")
                    return (f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}", "[]")

                # åå¤„ç†
                if hide_thoughts:
                    final_response = self._clean_thoughts(final_response)
                
                logger.info(f"ğŸ“¥ å“åº”é•¿åº¦: {len(final_response)}å­—ç¬¦")
                return final_response, json.dumps(response_context)
                
            except Exception as e:
                logger.exception("ğŸ’¥ æœªæ•è·å¼‚å¸¸")
                return (f"âŒ ç³»ç»Ÿé”™è¯¯: {str(e)}", "[]")
                
            finally:
                logger.debug(f"ğŸš¦ [{thread_name}] é€€å‡ºAPIä¸´ç•ŒåŒº")